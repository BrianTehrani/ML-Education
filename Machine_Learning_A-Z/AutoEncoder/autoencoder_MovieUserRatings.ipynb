{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae096e-7ee6-4538-9417-8055a758bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985733e-4106-4b28-863d-81285f2ea8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Importing the dataset\n",
    "\n",
    "# We won't be using this dataset.\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9acdd-eee1-470d-a1d5-79bbd585b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Preparing the training set and the test set\n",
    "\n",
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t', header = None)\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t', header = None)\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6316d2-ed47-4ceb-87df-251e2c30e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Getting the number of users and movies\n",
    "\n",
    "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))\n",
    "\n",
    "mins = int(min(min(training_set[:, 1], ), min(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d83533-0526-4a14-88bb-3a11aac17d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Converting the data into an array with users in lines and movies in columns\n",
    "\n",
    "def convertData(data:np.array):\n",
    "    \"\"\" \n",
    "        Convert training data into 2D matrix where row indicies equal users and\n",
    "        columns equal movie id. Cell reffers to user rating.\n",
    "        \n",
    "        Fill 0 for user who did not rate a movie.\n",
    "        \n",
    "        Information from training data:\n",
    "        Column 1: User -> min user = 1, max user = 943\n",
    "        Column 2: Movie -> min id = 1, max id = 1682\n",
    "        Column 3: Rating\n",
    "        Column 4: Timestamp\n",
    "    \n",
    "    \"\"\"\n",
    "    convert_data = []\n",
    "    for user in range(1, nb_users + 1):\n",
    "        zeros = [0] * (nb_movies + 1)\n",
    "        single_user_reviews = data[data[:, 0] == user]\n",
    "        for rating in single_user_reviews:\n",
    "            zeros[rating[1]] = rating[2]\n",
    "        zeros.pop(0) #no moview id that = 0\n",
    "        convert_data.append(zeros)\n",
    "    \n",
    "    return convert_data\n",
    "\n",
    "training_set = convertData(training_set)\n",
    "test_set = convertData(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a5473-3db2-42cc-991e-c36d36d675b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Converting the data into Torch tensors\n",
    "\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14997142-929a-4e55-9590-cdc555a1e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Creating the architecture of the Neural Network\n",
    "\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "       x = self.activation(self.fc1(x))\n",
    "       x = self.activation(self.fc2(x))\n",
    "       x = self.activation(self.fc3(x))\n",
    "       x = self.fc4(x)    \n",
    "       return x\n",
    "   \n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay= 0.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118f93d-3867-4aa4-85d5-fe1a9b30d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.7710038021536338\n",
      "epoch: 2 loss: 1.0967942641436759\n",
      "epoch: 3 loss: 1.0534919115681054\n",
      "epoch: 4 loss: 1.038341413930447\n",
      "epoch: 5 loss: 1.0307965015720548\n",
      "epoch: 6 loss: 1.0266279998988606\n",
      "epoch: 7 loss: 1.0237359782155178\n",
      "epoch: 8 loss: 1.022003730726132\n",
      "epoch: 9 loss: 1.0211101033059193\n",
      "epoch: 10 loss: 1.0198268054351565\n",
      "epoch: 11 loss: 1.0189284032221775\n",
      "epoch: 12 loss: 1.0182938190793258\n",
      "epoch: 13 loss: 1.0178759194903797\n",
      "epoch: 14 loss: 1.0176890740380709\n",
      "epoch: 15 loss: 1.0173441093582898\n",
      "epoch: 16 loss: 1.0168743334327013\n",
      "epoch: 17 loss: 1.0168435134454028\n",
      "epoch: 18 loss: 1.0165272311725169\n",
      "epoch: 19 loss: 1.0164157870613109\n",
      "epoch: 20 loss: 1.016310107251001\n",
      "epoch: 21 loss: 1.0157790972286351\n",
      "epoch: 22 loss: 1.0163073883870248\n",
      "epoch: 23 loss: 1.015571700391488\n",
      "epoch: 24 loss: 1.0159257136274675\n",
      "epoch: 25 loss: 1.0157679560151942\n",
      "epoch: 26 loss: 1.0158646468338834\n",
      "epoch: 27 loss: 1.0150759930836835\n",
      "epoch: 28 loss: 1.0152429804774221\n",
      "epoch: 29 loss: 1.0138728725826693\n",
      "epoch: 30 loss: 1.0124863423677986\n",
      "epoch: 31 loss: 1.0097561102904464\n",
      "epoch: 32 loss: 1.0095773707549072\n",
      "epoch: 33 loss: 1.005773927733994\n",
      "epoch: 34 loss: 1.0053516434941203\n",
      "epoch: 35 loss: 1.0017072185823857\n",
      "epoch: 36 loss: 0.9999131168808577\n",
      "epoch: 37 loss: 0.9982535806131955\n",
      "epoch: 38 loss: 0.9966014078532142\n",
      "epoch: 39 loss: 0.9940511096809617\n",
      "epoch: 40 loss: 0.9931542600876072\n",
      "epoch: 41 loss: 0.9906663281083272\n",
      "epoch: 42 loss: 0.9884393144548257\n",
      "epoch: 43 loss: 0.9887010456696392\n",
      "epoch: 44 loss: 0.9881087390223774\n",
      "epoch: 45 loss: 0.9831035040938666\n",
      "epoch: 46 loss: 0.9810083389512679\n",
      "epoch: 47 loss: 0.9768403758863893\n",
      "epoch: 48 loss: 0.9783374976795802\n",
      "epoch: 49 loss: 0.9719974679011075\n",
      "epoch: 50 loss: 0.9727632434389983\n",
      "epoch: 51 loss: 0.9710341277839828\n",
      "epoch: 52 loss: 0.9741118886003715\n",
      "epoch: 53 loss: 0.9715029061357646\n",
      "epoch: 54 loss: 0.9708464692476726\n",
      "epoch: 55 loss: 0.966719973048152\n",
      "epoch: 56 loss: 0.9652507165453195\n",
      "epoch: 57 loss: 0.9640389921645793\n",
      "epoch: 58 loss: 0.9633626341686106\n",
      "epoch: 59 loss: 0.9629094647029148\n",
      "epoch: 60 loss: 0.9615880512485823\n",
      "epoch: 61 loss: 0.9589644810549256\n",
      "epoch: 62 loss: 0.9583760073130982\n",
      "epoch: 63 loss: 0.9545133160118365\n",
      "epoch: 64 loss: 0.9548734257946528\n",
      "epoch: 65 loss: 0.9514328263733958\n",
      "epoch: 66 loss: 0.9514046156328576\n",
      "epoch: 67 loss: 0.9506996827510812\n",
      "epoch: 68 loss: 0.9511732285850355\n",
      "epoch: 69 loss: 0.948485467552277\n",
      "epoch: 70 loss: 0.948843623165175\n",
      "epoch: 71 loss: 0.9476209686130889\n",
      "epoch: 72 loss: 0.9498191438342205\n",
      "epoch: 73 loss: 0.9448824393097285\n",
      "epoch: 74 loss: 0.9482156723884255\n",
      "epoch: 75 loss: 0.945654807366979\n",
      "epoch: 76 loss: 0.9461773702630645\n",
      "epoch: 77 loss: 0.9437367582657573\n",
      "epoch: 78 loss: 0.9425820488221615\n",
      "epoch: 79 loss: 0.9418139788320102\n",
      "epoch: 80 loss: 0.9420556674975595\n",
      "epoch: 81 loss: 0.9408354547121263\n",
      "epoch: 82 loss: 0.9406839201480787\n",
      "epoch: 83 loss: 0.9394965025652025\n",
      "epoch: 84 loss: 0.939655308521996\n",
      "epoch: 85 loss: 0.9384756862274186\n",
      "epoch: 86 loss: 0.9387526467247816\n",
      "epoch: 87 loss: 0.9372220322643317\n",
      "epoch: 88 loss: 0.9383250656892402\n",
      "epoch: 89 loss: 0.9359800059001251\n",
      "epoch: 90 loss: 0.9375141310954976\n",
      "epoch: 91 loss: 0.9354686388706922\n",
      "epoch: 92 loss: 0.9366433852674333\n",
      "epoch: 93 loss: 0.9345951094381425\n",
      "epoch: 94 loss: 0.9356634405276636\n",
      "epoch: 95 loss: 0.9343125817995975\n",
      "epoch: 96 loss: 0.9352858255827041\n",
      "epoch: 97 loss: 0.9336225791213913\n",
      "epoch: 98 loss: 0.9345969455370841\n",
      "epoch: 99 loss: 0.9330140795030932\n",
      "epoch: 100 loss: 0.9401112784992842\n",
      "epoch: 101 loss: 0.939270847841145\n",
      "epoch: 102 loss: 0.9374739943683936\n",
      "epoch: 103 loss: 0.9370480367204308\n",
      "epoch: 104 loss: 0.9363031376249453\n",
      "epoch: 105 loss: 0.9366839007877998\n",
      "epoch: 106 loss: 0.9397444083237468\n",
      "epoch: 107 loss: 0.9398517953712119\n",
      "epoch: 108 loss: 0.9361316106706614\n",
      "epoch: 109 loss: 0.9344071619306301\n",
      "epoch: 110 loss: 0.9358300979579831\n",
      "epoch: 111 loss: 0.9359955076727285\n",
      "epoch: 112 loss: 0.9335260417033677\n",
      "epoch: 113 loss: 0.9337033217056678\n",
      "epoch: 114 loss: 0.9342666257975067\n",
      "epoch: 115 loss: 0.9322079423353065\n",
      "epoch: 116 loss: 0.9315455904914556\n",
      "epoch: 117 loss: 0.9293139408537199\n",
      "epoch: 118 loss: 0.9291346801764409\n",
      "epoch: 119 loss: 0.9293478071242267\n",
      "epoch: 120 loss: 0.9301481870135042\n",
      "epoch: 121 loss: 0.928662628829841\n",
      "epoch: 122 loss: 0.9289064436658234\n",
      "epoch: 123 loss: 0.9274878589273741\n",
      "epoch: 124 loss: 0.9303639315526719\n",
      "epoch: 125 loss: 0.9275210409319179\n",
      "epoch: 126 loss: 0.9286437915753238\n",
      "epoch: 127 loss: 0.925831825785269\n",
      "epoch: 128 loss: 0.9272276141633798\n",
      "epoch: 129 loss: 0.926062208624142\n",
      "epoch: 130 loss: 0.927268044120495\n",
      "epoch: 131 loss: 0.9253021423030573\n",
      "epoch: 132 loss: 0.9264000981695383\n",
      "epoch: 133 loss: 0.9245986255007401\n",
      "epoch: 134 loss: 0.9257554738036906\n",
      "epoch: 135 loss: 0.9240316254334372\n",
      "epoch: 136 loss: 0.9253185750642108\n",
      "epoch: 137 loss: 0.9234270705838246\n",
      "epoch: 138 loss: 0.924512817259222\n",
      "epoch: 139 loss: 0.9224438438228895\n",
      "epoch: 140 loss: 0.9237721870952763\n",
      "epoch: 141 loss: 0.9221549327061502\n",
      "epoch: 142 loss: 0.9232735669547952\n",
      "epoch: 143 loss: 0.92152666458008\n",
      "epoch: 144 loss: 0.9226433670189302\n",
      "epoch: 145 loss: 0.9213506546118561\n",
      "epoch: 146 loss: 0.9219508895248917\n",
      "epoch: 147 loss: 0.920562512791661\n",
      "epoch: 148 loss: 0.9213749970977655\n",
      "epoch: 149 loss: 0.9204324479589324\n",
      "epoch: 150 loss: 0.9212451679462266\n",
      "epoch: 151 loss: 0.9196969807070392\n",
      "epoch: 152 loss: 0.9206284241868655\n",
      "epoch: 153 loss: 0.9194280270180656\n",
      "epoch: 154 loss: 0.9203844706634405\n",
      "epoch: 155 loss: 0.9190491136571368\n",
      "epoch: 156 loss: 0.9197264151441116\n",
      "epoch: 157 loss: 0.9184305960970892\n",
      "epoch: 158 loss: 0.9193280698631435\n",
      "epoch: 159 loss: 0.9183688594751435\n",
      "epoch: 160 loss: 0.9191850934627266\n",
      "epoch: 161 loss: 0.91810790224072\n",
      "epoch: 162 loss: 0.9189320281987293\n",
      "epoch: 163 loss: 0.9174512405562107\n",
      "epoch: 164 loss: 0.9185515329423267\n",
      "epoch: 165 loss: 0.9171042952675854\n",
      "epoch: 166 loss: 0.918135279342005\n",
      "epoch: 167 loss: 0.916528066922086\n",
      "epoch: 168 loss: 0.9177743906368313\n",
      "epoch: 169 loss: 0.9165421874929766\n",
      "epoch: 170 loss: 0.9174917511651715\n",
      "epoch: 171 loss: 0.9160004119002809\n",
      "epoch: 172 loss: 0.9171848109530011\n",
      "epoch: 173 loss: 0.9159997996703197\n",
      "epoch: 174 loss: 0.9167431534166126\n",
      "epoch: 175 loss: 0.9157599647124548\n",
      "epoch: 176 loss: 0.9166102705824393\n",
      "epoch: 177 loss: 0.9154826601932984\n",
      "epoch: 178 loss: 0.9162186064853517\n",
      "epoch: 179 loss: 0.915029425289753\n",
      "epoch: 180 loss: 0.9157780897130732\n",
      "epoch: 181 loss: 0.9146375751079974\n",
      "epoch: 182 loss: 0.9158241550784813\n",
      "epoch: 183 loss: 0.9146850512747713\n",
      "epoch: 184 loss: 0.9152666520449835\n",
      "epoch: 185 loss: 0.9144499688481019\n",
      "epoch: 186 loss: 0.9152362751743518\n",
      "epoch: 187 loss: 0.9144488532771711\n",
      "epoch: 188 loss: 0.9149837413371479\n",
      "epoch: 189 loss: 0.9138721730308246\n",
      "epoch: 190 loss: 0.9145682164680474\n",
      "epoch: 191 loss: 0.9136931475760126\n",
      "epoch: 192 loss: 0.9143999167422767\n",
      "epoch: 193 loss: 0.9133467426438386\n",
      "epoch: 194 loss: 0.9141121414985545\n",
      "epoch: 195 loss: 0.9134475776843023\n",
      "epoch: 196 loss: 0.9142895671104033\n",
      "epoch: 197 loss: 0.9130787138871284\n",
      "epoch: 198 loss: 0.9137768234561098\n",
      "epoch: 199 loss: 0.9127919139472354\n",
      "epoch: 200 loss: 0.9138376850341011\n"
     ]
    }
   ],
   "source": [
    "#%% Training the SAE\n",
    "\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input_x = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input_x.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae.forward(input_x)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    \n",
    "    print('epoch: ' + str(epoch) + ' loss: '+ str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b02f3f-bece-4555-8717-e1c46689c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9482630600945305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%% Testing the SAE\n",
    "\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input_x = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input_x)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "        s += 1.\n",
    "\n",
    "print('test loss: '+ str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
