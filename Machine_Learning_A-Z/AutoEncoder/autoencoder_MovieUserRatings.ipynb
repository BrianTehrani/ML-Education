{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder\n",
    "\n",
    "*Project Description*\n",
    "- Develop a model to predict a users next movie rating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ae096e-7ee6-4538-9417-8055a758bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "import warnings\n",
    "\n",
    "# Set up device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below dataset is used to train autoencoder on 1 million user ratings.\n",
    "- Use dataset for systems that are able to train on larger datasets.\n",
    "- The current model is tested off a dataset containing 100k user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1985733e-4106-4b28-863d-81285f2ea8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "# users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "# ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Testing and Training sets. (100k user data)\n",
    "- 100k dataset has 5 versions of Train/Test splits.\n",
    "- Use first of these splits \"u1\" to train autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c9acdd-eee1-470d-a1d5-79bbd585b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "   0  1  2          3\n",
      "0  1  1  5  874965758\n",
      "1  1  2  3  876893171\n",
      "Column Keys: \n",
      " 0: Users \n",
      " 1: Movies \n",
      " 2: Ratings \n",
      " 3: Timestamps\n"
     ]
    }
   ],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t', header = None)\n",
    "print(f\"Training set:\\n{training_set.head(2)}\")\n",
    "print(\"Column Keys: \\n 0: Users \\n 1: Movies \\n 2: Ratings \\n 3: Timestamps\")\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t', header = None)\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the maximum value of users and movies.\n",
    "- Max could be located in either the training or the test set.\n",
    "- Therefore, find the max value in both training and test set, then return the max of the resut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f6316d2-ed47-4ceb-87df-251e2c30e6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users : 943\n",
      "Number of Movies: 1682\n"
     ]
    }
   ],
   "source": [
    "nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))\n",
    "print(f\"Number of Users : {nb_users}\")\n",
    "print(f\"Number of Movies: {nb_movies}\")\n",
    "\n",
    "mins = int(min(min(training_set[:, 1], ), min(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d83533-0526-4a14-88bb-3a11aac17d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertData(data:np.array):\n",
    "    \"\"\" \n",
    "        Convert training data into 2D matrix where row indicies equal users and\n",
    "        columns equal movie id. Cell reffers to user rating.\n",
    "        \n",
    "        Fill 0 for user who did not rate a movie.\n",
    "        \n",
    "        Information from training data:\n",
    "        Column 1: User -> min user = 1, max user = 943\n",
    "        Column 2: Movie -> min id = 1, max id = 1682\n",
    "        Column 3: Rating -> min rating = 0, max rating = 5\n",
    "        Column 4: Timestamp -> not used for training\n",
    "    \n",
    "    \"\"\"\n",
    "    convert_data = []\n",
    "    for user in range(1, nb_users + 1):\n",
    "\n",
    "        # Preallocate array with len of total movies in database\n",
    "        zeros = [0] * (nb_movies + 1)\n",
    "\n",
    "        # Gather all reviews a user has made\n",
    "        single_user_reviews = data[data[:, 0] == user]\n",
    "\n",
    "        # Fill loc zero with user rating that is in same column as movie id.\n",
    "        for rating in single_user_reviews:\n",
    "            zeros[rating[1]] = rating[2]\n",
    "        \n",
    "        # Remove first element in zeros because that column loc indicates a movie id of 0.\n",
    "        # In database there does not exist a movie is with 0. (min id = 1)\n",
    "        zeros.pop(0)\n",
    "        convert_data.append(zeros)\n",
    "    \n",
    "    return convert_data # (943, 1682) based of u1 dataset\n",
    "\n",
    "training_set = convertData(training_set)\n",
    "test_set = convertData(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about converted datasets:\n",
    "- A users row will contain many rattings with 0. This is because a user is not expected to review all moves in datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a59a5473-3db2-42cc-991e-c36d36d675b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "training_set = torch.tensor(training_set, dtype=torch.float, device=device)\n",
    "test_set = torch.tensor(training_set, dtype=torch.float, device=device)\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Develop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14997142-929a-4e55-9590-cdc555a1e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(nb_movies, 20),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.Linear(20, nb_movies)\n",
    "        )\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):   \n",
    "       return self.activation(self.layers(x))\n",
    "   \n",
    "sae = SAE().to(device)\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(sae.parameters(), lr = 0.001, weight_decay= 0.50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4118f93d-3867-4aa4-85d5-fe1a9b30d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 loss: 1.6024633166778433\n",
      "epoch: 40 loss: 1.6023816835638196\n",
      "epoch: 60 loss: 1.6023639545319674\n",
      "epoch: 80 loss: 1.6023568041301266\n",
      "epoch: 100 loss: 1.6023530621869746\n",
      "epoch: 120 loss: 1.6023508014553458\n",
      "epoch: 140 loss: 1.602349296990143\n",
      "epoch: 160 loss: 1.6023482383991303\n",
      "epoch: 180 loss: 1.6023474541859781\n",
      "epoch: 200 loss: 1.6023468486579169\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 200\n",
    "\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    \n",
    "    sae.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Count number of users that rated 1 movie\n",
    "    # Do not count users who did not rate any movie for memory optimization\n",
    "    num_users_who_rate_at_least_1_movie = 0. # Will use to compute RMS error\n",
    "\n",
    "    for id_user in range(nb_users):\n",
    "        input_x = training_set[id_user].unsqueeze(0).requires_grad_()\n",
    "        target = input_x.clone().detach() # removes gradient tracking from target\n",
    "\n",
    "        # Only train on data where user rated at least one movie.\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            \n",
    "            output = sae(input_x)\n",
    "           \n",
    "            \"\"\" \n",
    "                Below is necessary for optimization and model performance.\n",
    "\n",
    "                If a user did not rate a movie and the model predicts their rating to be high,\n",
    "                the model will calulate this RMS error and will adjust inappropriately:\n",
    "\n",
    "                ex/ User did not rate movie = 0 \n",
    "                    Model predicts = 3\n",
    "\n",
    "                    This result will carry over to loss and assume a loss of 3. \n",
    "                    Model loss will theerefore have a difficult time decreasing.\n",
    "            \"\"\"\n",
    "            # output[target == 0] = 0\n",
    "            output_2 = torch.where(target == 0, 0., output).requires_grad_()\n",
    "           \n",
    "            loss = loss_fn(output_2, target)\n",
    "            # if id_user % 100 == 0:\n",
    "            #     print(f\"Sum of target: {torch.sum(target.data > 0)}  -  Difference: {torch.sum(torch.subtract(target.data, output_2) > 0)}\")\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            \n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
    "            num_users_who_rate_at_least_1_movie += 1.\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print('epoch: ' + str(epoch) + ' loss: '+ str(train_loss/num_users_who_rate_at_least_1_movie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b02f3f-bece-4555-8717-e1c46689c358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.6023468302387454\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    sae.eval()\n",
    "    test_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input_x = test_set[id_user].unsqueeze(0).requires_grad_()\n",
    "        target = input_x.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input_x)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = loss_fn(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            test_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "            s += 1.\n",
    "\n",
    "print('test loss: '+ str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  print(sae(training_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model takes a while to train on most systems.\n",
    "Save / Load model state after training in case errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sae, r'/model_sae.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
